{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.01 - Modelling COB Peaks Timeseries\n",
    "Using the peaks to assess the distribution of the data is a helpful approach to understand the distribution of meal intake over time. It has made it easier to assess the correctness of the data mapping to a daily pattern, especially given the issues with datetimes not aligning to the timezones that they are in. We'll now use the peaks to identify the COB values we are interested in modelling. The aim is to be able to assess what a standard day looks like and whether it is possible to idenfity where days are not standard, which may be due to errors in the data or due to the individual having a different pattern of meal intake. The peaks will be used to identify the COB values that are relevant for modelling, and then the timeseries will be used to assess the distribution and amplitude of those values over time. We will use the 15-minute resampled data here and focus on one of the candidates with the most defined distributions that shows a 3-meal intake clearly."
   ],
   "id": "94b63cf85ae0a231"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:38:29.544395Z",
     "start_time": "2025-06-02T15:38:14.861414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import CrostonClassic, CrostonSBA, CrostonOptimized, TSB\n",
    "\n",
    "from src.cob_analysis import Cob\n",
    "from src.data_processing.read import read_profile_offsets_csv\n",
    "from src.configurations import Configuration\n",
    "from src.time_series_analysis import run_adf, p_q_result, ts_dist_plot, ts_plot, split_ts, ts_plot_cfs"
   ],
   "id": "90192c63050ef5b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\.env\\master_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T10:18:30.148001Z",
     "start_time": "2025-06-02T10:17:51.409915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logger.remove()\n",
    "\n",
    "candidates = [13029224, 21946407, 27700103, 32407882, 41131654, 42360672, 67208817, 74175219, 79526193, 86025410, 95851255, 96254963, 96805916, 97417885]\n",
    "individual = 41131654\n",
    "args = {'height': 15, 'distance': 5, 'suppress': False}\n",
    "config = Configuration()\n",
    "\n",
    "profile_offsets = read_profile_offsets_csv(config)\n",
    "\n",
    "cob = Cob()\n",
    "cob.read_interim_data(file_name='15min_iob_cob_bg', sampling_rate=15)\n",
    "df_all = cob.process_one_tz_individuals(profile_offsets, args)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 786757\n",
      "Number of people: 133\n",
      "Systems used: \t['OpenAPS']\n",
      "Categories (1, object): ['OpenAPS']\n",
      "From 120 IDs requested processing, ignored 22 individuals not found in dataset, leaving 98 processed records.\n",
      "The following stats are based on parameters h=15 and d=5:\n",
      "\tNumber of records: 2637045\n",
      "\tNumber of days with peaks: 7923\n",
      "\tNumber of peaks: 19539\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data has a 'cob max' column that we need to transform such that it only holds the values that are relevant for modelling. The peaks will be used to identify the COB values that are used for features. That removes any noise from other values. Note, the imputed values are not used and would be irrelevant anyway, given that we are focussing purely on the values that are peaks only. These would alway be original values.",
   "id": "74433cdf9f17da9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:18:31.725603Z",
     "start_time": "2025-06-02T10:18:31.653488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df_all.loc[individual].copy()\n",
    "df.index.freq = str(cob.sampling_rate) + 'min'  # Avoid FutureWarning"
   ],
   "id": "716a0a2692d29d9b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:18:35.301949Z",
     "start_time": "2025-06-02T10:18:31.730094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Running a quick ADF test to check if the data is stationary\n",
    "run_adf(df['cob max'])"
   ],
   "id": "58ec65de92fb5971",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Test Results\n",
      "Null Hypothesis: The series has a unit root (non-stationary)\n",
      "ADF-Statistic: -26.97359102321359\n",
      "P-Value: 0.0\n",
      "Number of lags: 53\n",
      "Number of observations: 36363\n",
      "Critical Values: {'1%': np.float64(-3.9590189861090983), '5%': np.float64(-3.4106107449475744), '10%': np.float64(-3.1271211082131924)}\n",
      "Note: If P-Value is smaller than 0.05, we reject the null hypothesis and the series is stationary.\n",
      "A more negative test statistic indicates stronger evidence against the null hypothesis.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This shows that the data is showing stationarity, despite the fact that we can see the clustering of peaks on a overimposed days. It is unsurprising considering: a) the data is not consistently entered by the individual, and b) we know the data is sparse, which combined will impact immediate lags. Therefore, we need to extend our approach to appreciate the seasonality, and are able to do so with  using the SARIMA model.\n",
    "\n",
    "SARIMA is a seasonal extension of ARIMA, which is a popular time series forecasting method. It is particularly useful for data that exhibits seasonality, as it incorporates both non-seasonal and seasonal components. The SARIMA model can be expressed as:\n",
    "$$\n",
    "SARIMA(p, d, q)(P, D, Q)_s\n",
    "$$\n",
    "where:\n",
    "- p: order of the non-seasonal autoregressive part\n",
    "- d: degree of differencing for the non-seasonal part\n",
    "-  q: order of the non-seasonal moving average part\n",
    "- P: order of the seasonal autoregressive part\n",
    "- D: degree of differencing for the seasonal part\n",
    "- Q: order of the seasonal moving average part\n",
    "- s: length of the seasonal cycle (e.g., 96 for 15-minute intervals over a day)\n",
    "- The SARIMA model can be used to capture both the short-term and long-term patterns in the data, making it suitable for forecasting and understanding the underlying trends.\n",
    "\n",
    "The model is defined by three main parameters: p, d, q (non-seasonal) and P, D, Q (seasonal), along with the seasonal period s. This seasonal period would be a day, considering we expect a person's eating pattern to be diurnal. The parameters would need to respond to the sampling rate. That used for example is 15 minutes, and the period would be 96 (24 hours * 60 minutes / 15 minutes).\n",
    "\n",
    "However, with careful consideration, for this time series we don't witness a decay in the data points we are using as our random variable, therefore we should look to another method for this use. As a baseline, we'll look at Croston's Method (or SBA). `StatsForecast` offers a library of Croston-based (and similar) methods. The classic Croston's formulation is:\n",
    "- $Y_t$ : Carbohydrate intake at time $t$.\n",
    "- $Z_j$ : The $j^{th}$ non-zero carbohydrate intake value.\n",
    "- $X_j$ : The $j^{th}$ inter-arrival time (number of periods between non-zero values).\n",
    "\n",
    "Simple Exponential Smoothing (SES) for $Z_j$ and $X_j$:\n",
    "$$s_j = \\alpha \\cdot Z_j + (1 - \\alpha) \\cdot s_{j-1}$$\n",
    "where $\\alpha$ is the smoothing parameter (0 < $\\alpha$ < 1).\n",
    "$$a_j = \\beta \\cdot X_j + (1 - \\beta) \\cdot a_{j-1}$$\n",
    "\n",
    "The forecast for the next non-zero value is given by:\n",
    "$$\\hat{Y}_{t+h} = \\frac{s_N}{a_N}$$\n",
    "where $h$ is the forecast horizon and $N$ is the total number of non-zero intake events that have occurred in historical data up to the current time $t$. $s_N$ is the latest smoothed demand size, which is the exponentially smoothed average of all N non-zero demand values encountered in the historical data. $a_N$ is the latest smoothed interval, which is the exponentially smoothed average of all $N$ intervals between these non-zero demand occurrences.\n",
    "\n",
    "Lets implement and see what the results look like. We will use the `StatsForecast` library to implement Croston's Method, and then plot the results to see how well it captures the peaks in the data.\n"
   ],
   "id": "2100b4af3ebd48ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T08:06:19.000637Z",
     "start_time": "2025-06-03T08:06:18.576299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# StatsForecast expects a specific DataFrame format or use the parameters. Here, we're aligned the column names to match the expected format: 'unique_id', 'ds' (datetime), 'y' (values)\n",
    "df_croston = df.copy()\n",
    "df_croston['y'] = np.where(df_croston['peak'] == 1, df_croston['cob max'], 0)\n",
    "df_croston = (df_croston.\n",
    "              reset_index().reset_index().\n",
    "              rename(columns={'index': 'unique_id', 'datetime': 'ds'}).\n",
    "              drop(columns=['cob max', 'day', 'time', 'cob interpolate', 'peak', 'offset']))\n",
    "\n",
    "def print_df_characteristics(df_croston):\n",
    "    print(\"Sample Data Head (StatsForecast format):\")\n",
    "    print(df_croston.head(10))\n",
    "    print(\"\\nNumber of non-zero observations:\", (df_croston['y'] > 0).sum())\n",
    "    print(\"Total observations:\", len(df_croston))\n",
    "    print(\"Sparsity (proportion of zeros):\", np.mean(df_croston['y'] == 0))\n",
    "\n",
    "print_df_characteristics(df_croston)"
   ],
   "id": "729b45eac6d628ba",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'peak'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\.env\\master_project\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3804\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3805\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3806\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mindex.pyx:167\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mindex.pyx:196\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mKeyError\u001B[39m: 'peak'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# StatsForecast expects a specific DataFrame format or use the parameters. Here, we're aligned the column names to match the expected format: 'unique_id', 'ds' (datetime), 'y' (values)\u001B[39;00m\n\u001B[32m      2\u001B[39m df_croston = df.copy()\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m df_croston[\u001B[33m'\u001B[39m\u001B[33my\u001B[39m\u001B[33m'\u001B[39m] = np.where(\u001B[43mdf_croston\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mpeak\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m == \u001B[32m1\u001B[39m, df_croston[\u001B[33m'\u001B[39m\u001B[33mcob max\u001B[39m\u001B[33m'\u001B[39m], \u001B[32m0\u001B[39m)\n\u001B[32m      4\u001B[39m df_croston = (df_croston.\n\u001B[32m      5\u001B[39m               reset_index().reset_index().\n\u001B[32m      6\u001B[39m               rename(columns={\u001B[33m'\u001B[39m\u001B[33mindex\u001B[39m\u001B[33m'\u001B[39m: \u001B[33m'\u001B[39m\u001B[33munique_id\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdatetime\u001B[39m\u001B[33m'\u001B[39m: \u001B[33m'\u001B[39m\u001B[33mds\u001B[39m\u001B[33m'\u001B[39m}).\n\u001B[32m      7\u001B[39m               drop(columns=[\u001B[33m'\u001B[39m\u001B[33mcob max\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mday\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mtime\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mcob interpolate\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mpeak\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33moffset\u001B[39m\u001B[33m'\u001B[39m]))\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprint_df_characteristics\u001B[39m(df_croston):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\.env\\master_project\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4100\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.nlevels > \u001B[32m1\u001B[39m:\n\u001B[32m   4101\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._getitem_multilevel(key)\n\u001B[32m-> \u001B[39m\u001B[32m4102\u001B[39m indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4103\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[32m   4104\u001B[39m     indexer = [indexer]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\.env\\master_project\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3807\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   3808\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc.Iterable)\n\u001B[32m   3809\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[32m   3810\u001B[39m     ):\n\u001B[32m   3811\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[32m-> \u001B[39m\u001B[32m3812\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m   3814\u001B[39m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[32m   3815\u001B[39m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[32m   3816\u001B[39m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[32m   3817\u001B[39m     \u001B[38;5;28mself\u001B[39m._check_indexing_error(key)\n",
      "\u001B[31mKeyError\u001B[39m: 'peak'"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The result shows our data to be very sparse, as we would expect. Almost 97% zero-values. This can cause issues with the Croston's Method, especially if the smoothing parameters are not set correctly or if the data is too sparse, leading to forecasts that are too low or even zero due to numerical precision issues.\n",
    "\n",
    "Some notes on the implementation using StatsForecast:\n",
    "- For Croston's, you generally don't set alpha/beta directly in `CrostonClassic`/`SBA` as they use default values or optimized values. `CrostonOptimized` allows you to tune alpha and beta, or it can optimize them.\n",
    "- The `TSB` model requires you to specify smoothing parameters (`alpha_d` for demand and `alpha_p` for period), which can be tuned based on your data characteristics."
   ],
   "id": "e4c28cf5c418e588"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T20:46:50.665536Z",
     "start_time": "2025-06-02T20:46:24.760503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_croston_models(df, sampling_rate=15):\n",
    "    \"\"\"\n",
    "    Run Croston's models on the provided DataFrame.\n",
    "    Returns a DataFrame with forecasts for each model.\n",
    "    \"\"\"\n",
    "    intervals_per_day = 24 * 60 // sampling_rate\n",
    "\n",
    "    models = [\n",
    "        CrostonClassic(),  # Classic Croston's with default alpha=0.1 for both\n",
    "        CrostonSBA(),      # Croston's with Syntetos-Boylan Approximation (bias-corrected)\n",
    "        CrostonOptimized(),# Optimized Croston's (finds best alpha/beta within a range)\n",
    "        TSB(alpha_d=0.2, alpha_p=0.2) # TSB model, requires smoothing parameters\n",
    "    ]\n",
    "\n",
    "    sf = StatsForecast(\n",
    "        models=models,\n",
    "        freq=str(sampling_rate)+'min',\n",
    "        n_jobs=-1 # Use all available CPU cores for parallel processing\n",
    "    )\n",
    "\n",
    "    sf.fit(df=df)\n",
    "\n",
    "    return sf.predict(h=intervals_per_day)  # Predict for the next 96 intervals (1 day), returns df\n",
    "\n",
    "def print_croston_forecasts(df, sampling_rate=15):\n",
    "    \"\"\"\n",
    "    Print the forecasts from Croston's models.\n",
    "    \"\"\"\n",
    "    intervals_per_day = 24 * 60 // sampling_rate\n",
    "\n",
    "    print(f\"\\nCrostonClassic Forecast (per 15-min interval): {df['CrostonClassic'].iloc[0]:.4f}\")\n",
    "    print(f\"CrostonSBA Forecast (per 15-min interval): {df['CrostonSBA'].iloc[0]:.4f}\")\n",
    "    print(f\"CrostonOptimized Forecast (per 15-min interval): {df['CrostonOptimized'].iloc[0]:.4f}\")\n",
    "    print(f\"TSB Forecast (per 15-min interval): {df['TSB'].iloc[0]:.4f}\")\n",
    "\n",
    "    # Calculate average daily intake for each model\n",
    "    for model_name in df.columns[2:]: # Skip 'unique_id' and 'ds'\n",
    "        daily_avg = df[model_name].iloc[0] * intervals_per_day\n",
    "        print(f\"Average Daily Forecast ({model_name}): {daily_avg:.2f} units\")\n",
    "\n",
    "forecast_df = run_croston_models(df_croston)\n",
    "print_croston_forecasts(forecast_df)"
   ],
   "id": "1753288a109935d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrostonClassic Forecast (per 15-min interval): 0.0000\n",
      "CrostonSBA Forecast (per 15-min interval): 0.0000\n",
      "CrostonOptimized Forecast (per 15-min interval): 0.0000\n",
      "TSB Forecast (per 15-min interval): 0.0000\n",
      "Average Daily Forecast (CrostonClassic): 0.00 units\n",
      "Average Daily Forecast (CrostonSBA): 0.00 units\n",
      "Average Daily Forecast (CrostonOptimized): 0.00 units\n",
      "Average Daily Forecast (TSB): 0.00 units\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The zero forecasts may be for a number of reasons:\n",
    "- The data is too sparse, leading to numerical precision issues.\n",
    "- The smoothing parameters are not set correctly, leading to forecasts that are too low or even zero.\n",
    "- The model is not able to capture the intermittent nature of the data, leading to forecasts that are not representative of the underlying demand.\n",
    "- The model is not able to capture the seasonality of the data, leading to forecasts that are not representative of the underlying demand.\n",
    "- Initialisation of $s_0$ and $a_0$ in Croston's Method can lead to zero forecasts if the initial values are not set correctly.\n",
    "\n",
    "Following previous visual checks, we have noted with this individual that they have a period of days in which they have no peaks, and therefore no data to model. This is likely to be the cause of the zero forecasts. We can check this by looking at the number of non-zero observations in the forecast data. We've created a function to remove days with zero intake from the data, so lets apply this and see if that changes the forecasts."
   ],
   "id": "7570e08f1d91a11d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T20:47:18.618857Z",
     "start_time": "2025-06-02T20:47:18.484242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.cob_analysis import remove_zero_or_null_days\n",
    "print(f'Before removing zero intake days: {len(df.groupby(\"day\").size())}')\n",
    "df_croston = df_croston.set_index('ds')  # Ensure 'ds' is the index for time series operations\n",
    "df_croston = remove_zero_or_null_days(df=df_croston, value_col='y')\n",
    "print(f'After removing zero intake days: {len(df_croston.groupby(df_croston.index.date).size())}')\n",
    "df_croston = df_croston.reset_index()  # Reset index to keep 'ds' as a column\n",
    "print(df_croston.info())"
   ],
   "id": "91c91fa75878ee26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing zero intake days: 380\n",
      "After removing zero intake days: 365\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34977 entries, 0 to 34976\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   ds         34977 non-null  datetime64[ns]\n",
      " 1   unique_id  34977 non-null  int64         \n",
      " 2   y          34977 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1)\n",
      "memory usage: 819.9 KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T20:48:02.750222Z",
     "start_time": "2025-06-02T20:47:37.940154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "forecast_df = run_croston_models(df_croston)\n",
    "print_croston_forecasts(forecast_df)"
   ],
   "id": "f174ea1bb7cfb54a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrostonClassic Forecast (per 15-min interval): 0.0000\n",
      "CrostonSBA Forecast (per 15-min interval): 0.0000\n",
      "CrostonOptimized Forecast (per 15-min interval): 0.0000\n",
      "TSB Forecast (per 15-min interval): 0.0000\n",
      "Average Daily Forecast (CrostonClassic): 0.00 units\n",
      "Average Daily Forecast (CrostonSBA): 0.00 units\n",
      "Average Daily Forecast (CrostonOptimized): 0.00 units\n",
      "Average Daily Forecast (TSB): 0.00 units\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This has not produced the expected results but this can be explained by the fact that - even though the gaps have been effectively removed - Croston's method will still count the gap in its inter-arrival time, drastically inflating the $a_N$ and pushing the prediction down, given it is the denominator (and especially where $\\alpha$ and $\\beta$ are small). A function has been created that splits the dataframe into a list of separate ones based on a threshold of day gaps, set here to 3.",
   "id": "297c768650e2bb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:52:36.613538Z",
     "start_time": "2025-06-03T07:52:36.280307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.cob_analysis import split_on_time_gaps\n",
    "\n",
    "split_dfs = split_on_time_gaps(df=df_croston.set_index('ds'), value_col='y', days_threshold=3)\n",
    "\n",
    "print(f'Number of split dfs: {len(split_dfs)}')\n",
    "for i, df in enumerate(split_dfs):\n",
    "    print(f'Split DataFrame {i+1} - Number of days: {len(df.groupby(df.index.date).size())}, Start: {df.index.min()}, End: {df.index.max()}')"
   ],
   "id": "64993ba4b094b596",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of split dfs: 2\n",
      "Split DataFrame 1 - Number of days: 2, Start: 2019-09-01 01:15:00, End: 2019-09-02 23:45:00\n",
      "Split DataFrame 2 - Number of days: 363, Start: 2019-09-16 00:00:00, End: 2020-09-14 09:15:00\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:59:04.474923Z",
     "start_time": "2025-06-03T07:58:10.250733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "forecast_df = run_croston_models(split_dfs[1].reset_index('ds'))\n",
    "print_croston_forecasts(forecast_df)"
   ],
   "id": "4eb5987a809fc713",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrostonClassic Forecast (per 15-min interval): 0.0000\n",
      "CrostonSBA Forecast (per 15-min interval): 0.0000\n",
      "CrostonOptimized Forecast (per 15-min interval): 0.0000\n",
      "TSB Forecast (per 15-min interval): 0.0000\n",
      "Average Daily Forecast (CrostonClassic): 0.00 units\n",
      "Average Daily Forecast (CrostonSBA): 0.00 units\n",
      "Average Daily Forecast (CrostonOptimized): 0.00 units\n",
      "Average Daily Forecast (TSB): 0.00 units\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # --- 5. Visualization (Optional) ---\n",
    "# # Merge historical data with forecasts for plotting\n",
    "# # The forecast for intermittent models is a constant line\n",
    "# plot_df = pd.concat([df_croston, forecast_df.drop(columns='unique_id')], axis=0)\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.plot(plot_df['ds'], plot_df['y'], label='Historical Carb Intake', alpha=0.7)\n",
    "# plt.plot(forecast_df['ds'], forecast_df['CrostonSBA'], label='CrostonSBA Forecast', color='red', linestyle='--')\n",
    "# plt.plot(forecast_df['ds'], forecast_df['TSB'], label='TSB Forecast', color='green', linestyle=':')\n",
    "# plt.title('Carbohydrate Intake: Historical Data vs. Intermittent Demand Forecasts')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Carbohydrate Intake (units)')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# # Zoomed-in plot to see the intermittent nature better\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.plot(plot_df['ds'].tail(intervals_per_day * 5), plot_df['y'].tail(intervals_per_day * 5), label='Historical Carb Intake', alpha=0.7)\n",
    "# plt.plot(forecast_df['ds'], forecast_df['CrostonSBA'], label='CrostonSBA Forecast', color='red', linestyle='--')\n",
    "# plt.plot(forecast_df['ds'], forecast_df['TSB'], label='TSB Forecast', color='green', linestyle=':')\n",
    "# plt.title('Carbohydrate Intake (Last 5 Days): Historical Data vs. Intermittent Demand Forecasts')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Carbohydrate Intake (units)')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ],
   "id": "634cdd0e6924a983"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "99be2f33495ef753"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:31:54.383901Z",
     "start_time": "2025-06-02T16:31:53.324987Z"
    }
   },
   "cell_type": "code",
   "source": "forecast_df.describe()",
   "id": "f29b99832a230d72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          unique_id                             ds  CrostonClassic  \\\n",
       "count  3.496032e+06                        3496032    3.496032e+06   \n",
       "mean   1.820800e+04  2020-03-09 05:22:30.000000512    1.510613e+00   \n",
       "min    0.000000e+00            2019-09-01 01:30:00    0.000000e+00   \n",
       "25%    9.104000e+03            2019-12-05 09:15:00    0.000000e+00   \n",
       "50%    1.820800e+04            2020-03-09 05:22:30    0.000000e+00   \n",
       "75%    2.731200e+04            2020-06-12 01:30:00    0.000000e+00   \n",
       "max    3.641600e+04            2020-09-15 09:15:00    1.200000e+02   \n",
       "std    1.051268e+04                            NaN    8.908414e+00   \n",
       "\n",
       "         CrostonSBA  CrostonOptimized           TSB  \n",
       "count  3.496032e+06      3.496032e+06  3.496032e+06  \n",
       "mean   1.435083e+00      1.510613e+00  1.510613e+00  \n",
       "min    0.000000e+00      0.000000e+00  0.000000e+00  \n",
       "25%    0.000000e+00      0.000000e+00  0.000000e+00  \n",
       "50%    0.000000e+00      0.000000e+00  0.000000e+00  \n",
       "75%    0.000000e+00      0.000000e+00  0.000000e+00  \n",
       "max    1.140000e+02      1.200000e+02  1.200000e+02  \n",
       "std    8.462993e+00      8.908414e+00  8.908414e+00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>CrostonClassic</th>\n",
       "      <th>CrostonSBA</th>\n",
       "      <th>CrostonOptimized</th>\n",
       "      <th>TSB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.496032e+06</td>\n",
       "      <td>3496032</td>\n",
       "      <td>3.496032e+06</td>\n",
       "      <td>3.496032e+06</td>\n",
       "      <td>3.496032e+06</td>\n",
       "      <td>3.496032e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.820800e+04</td>\n",
       "      <td>2020-03-09 05:22:30.000000512</td>\n",
       "      <td>1.510613e+00</td>\n",
       "      <td>1.435083e+00</td>\n",
       "      <td>1.510613e+00</td>\n",
       "      <td>1.510613e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2019-09-01 01:30:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.104000e+03</td>\n",
       "      <td>2019-12-05 09:15:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.820800e+04</td>\n",
       "      <td>2020-03-09 05:22:30</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.731200e+04</td>\n",
       "      <td>2020-06-12 01:30:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.641600e+04</td>\n",
       "      <td>2020-09-15 09:15:00</td>\n",
       "      <td>1.200000e+02</td>\n",
       "      <td>1.140000e+02</td>\n",
       "      <td>1.200000e+02</td>\n",
       "      <td>1.200000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.051268e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.908414e+00</td>\n",
       "      <td>8.462993e+00</td>\n",
       "      <td>8.908414e+00</td>\n",
       "      <td>8.908414e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
