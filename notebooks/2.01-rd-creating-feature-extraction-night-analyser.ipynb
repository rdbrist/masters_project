{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction at Time Series Level for Clustering Analysis\n",
    "The following assumes that we have now selected a set of candidates where the night periods have been preprocessed to ensure that they are consistent and complete, with a consistent start and end time and candidates have been selected based broadly that they maximise the number of complete nights (determined by having zero missing intervals)."
   ],
   "id": "b7ecdbe2c50c63e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T11:02:04.965097Z",
     "start_time": "2025-06-15T11:01:49.883833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from tsfresh import extract_features, extract_relevant_features, feature_selection\n",
    "from tsfresh.utilities.dataframe_functions import impute, roll_time_series\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters, EfficientFCParameters\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from src.helper import check_df_index\n",
    "from datetime import time\n",
    "from src.configurations import Configuration"
   ],
   "id": "49dbd4915bc09a0d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-06-15 12:02:04.666\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.config\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m11\u001B[0m - \u001B[1mPROJ_ROOT path is: C:\\Users\\ross\\OneDrive\\Documents\\Masters\\Project\\masters_project\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T11:25:57.468211Z",
     "start_time": "2025-06-15T11:25:57.216290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assumes pipeline has been run to provide the final filtered DataFrame config.final_filtered_csv\n",
    "config = Configuration()\n",
    "df = pd.read_csv(config.final_filtered_csv)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.set_index(['id', 'datetime'])\n",
    "df.info()"
   ],
   "id": "a4a79d22414209f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 7694 entries, (np.int64(41131654), Timestamp('2019-09-12 18:00:00')) to (np.int64(86025410), Timestamp('2017-05-03 10:30:00'))\n",
      "Data columns (total 18 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   iob mean   7694 non-null   float64\n",
      " 1   cob mean   7680 non-null   float64\n",
      " 2   bg mean    7694 non-null   float64\n",
      " 3   iob min    7694 non-null   float64\n",
      " 4   cob min    7680 non-null   float64\n",
      " 5   bg min     7694 non-null   float64\n",
      " 6   iob max    7694 non-null   float64\n",
      " 7   cob max    7680 non-null   float64\n",
      " 8   bg max     7694 non-null   float64\n",
      " 9   iob std    6575 non-null   float64\n",
      " 10  cob std    6544 non-null   float64\n",
      " 11  bg std     6575 non-null   float64\n",
      " 12  iob count  7694 non-null   int64  \n",
      " 13  cob count  7694 non-null   int64  \n",
      " 14  bg count   7694 non-null   int64  \n",
      " 15  offset     7694 non-null   int64  \n",
      " 16  day        7694 non-null   object \n",
      " 17  time       7694 non-null   object \n",
      "dtypes: float64(12), int64(4), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T11:26:14.309286Z",
     "start_time": "2025-06-15T11:26:14.232106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nan_counts = df.isna().sum()\n",
    "print(\"Total NaNs per column:\")\n",
    "print(nan_counts)"
   ],
   "id": "f245dc99fbc678b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaNs per column:\n",
      "iob mean        0\n",
      "cob mean       14\n",
      "bg mean         0\n",
      "iob min         0\n",
      "cob min        14\n",
      "bg min          0\n",
      "iob max         0\n",
      "cob max        14\n",
      "bg max          0\n",
      "iob std      1119\n",
      "cob std      1150\n",
      "bg std       1119\n",
      "iob count       0\n",
      "cob count       0\n",
      "bg count        0\n",
      "offset          0\n",
      "day             0\n",
      "time            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T11:29:34.305943Z",
     "start_time": "2025-06-15T11:29:34.140478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_isna = df[df['cob mean'].isna()]\n",
    "print('Count of intervals with NaNs for COB columns, i.e.e missing COB data.')\n",
    "df_isna.groupby(by=['id','day'])['day'].count() # Check rows with NaNs in 'cob mean'"
   ],
   "id": "361da8f9ad3b0240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of intervals with NaNs for COB columns, i.e.e missing COB data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id        day       \n",
       "79526193  2017-05-25    1\n",
       "          2017-05-27    2\n",
       "          2017-05-28    4\n",
       "          2017-05-30    1\n",
       "          2017-06-13    1\n",
       "          2017-06-18    2\n",
       "86025410  2017-04-03    1\n",
       "          2017-04-09    2\n",
       "Name: day, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T11:29:37.103611Z",
     "start_time": "2025-06-15T11:29:37.026696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count number of consecutive NaNs in 'cob mean' column\n",
    "df_reset = df_isna.reset_index().sort_values(['id', 'datetime'])\n",
    "df_reset['time_diff'] = df_reset.groupby('id')['datetime'].diff().dt.total_seconds() / 60\n",
    "df_reset.groupby('id')['time_diff'].apply(lambda x: (x == 30).sum())"
   ],
   "id": "bd374a739a69a37d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "79526193    2\n",
       "86025410    1\n",
       "Name: time_diff, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are only three instances where these NaNs cover are greater than 30 minutes, which is the minimum interval between two consecutive COB measurements. This is very few and means that we can proceed with feature extraction and imputation of these NaNs.",
   "id": "a2b89546e888f9c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T11:04:29.511638Z",
     "start_time": "2025-06-15T11:04:29.264377Z"
    }
   },
   "cell_type": "code",
   "source": "extracted_features = extract_features(df.reset_index(), column_id='id', column_sort='datetime')",
   "id": "7725b4e6aa6b850b",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column must not contain NaN values: cob mean",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m extracted_features = \u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_all_selected\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreset_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_id\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mid\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_sort\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdatetime\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\.env\\master_project\\Lib\\site-packages\\tsfresh\\feature_extraction\\extraction.py:164\u001B[39m, in \u001B[36mextract_features\u001B[39m\u001B[34m(timeseries_container, default_fc_parameters, kind_to_fc_parameters, column_id, column_sort, column_kind, column_value, chunksize, n_jobs, show_warnings, disable_progressbar, impute_function, profile, profiling_filename, profiling_sorting, distributor, pivot)\u001B[39m\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    162\u001B[39m     warnings.simplefilter(\u001B[33m\"\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m164\u001B[39m result = \u001B[43m_do_extraction\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeseries_container\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    166\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumn_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumn_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    167\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumn_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumn_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    168\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumn_kind\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumn_kind\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    169\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumn_sort\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumn_sort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    170\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[43m    \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    172\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdisable_progressbar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisable_progressbar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m    \u001B[49m\u001B[43mshow_warnings\u001B[49m\u001B[43m=\u001B[49m\u001B[43mshow_warnings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdefault_fc_parameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdefault_fc_parameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkind_to_fc_parameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkind_to_fc_parameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdistributor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdistributor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpivot\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpivot\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    180\u001B[39m \u001B[38;5;66;03m# Impute the result if requested\u001B[39;00m\n\u001B[32m    181\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m impute_function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\.env\\master_project\\Lib\\site-packages\\tsfresh\\feature_extraction\\extraction.py:260\u001B[39m, in \u001B[36m_do_extraction\u001B[39m\u001B[34m(df, column_id, column_value, column_kind, column_sort, default_fc_parameters, kind_to_fc_parameters, n_jobs, chunk_size, disable_progressbar, show_warnings, distributor, pivot)\u001B[39m\n\u001B[32m    193\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_do_extraction\u001B[39m(\n\u001B[32m    194\u001B[39m     df,\n\u001B[32m    195\u001B[39m     column_id,\n\u001B[32m   (...)\u001B[39m\u001B[32m    206\u001B[39m     pivot,\n\u001B[32m    207\u001B[39m ):\n\u001B[32m    208\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    209\u001B[39m \u001B[33;03m    Wrapper around the _do_extraction_on_chunk, which calls it on all chunks in the data frame.\u001B[39;00m\n\u001B[32m    210\u001B[39m \u001B[33;03m    A chunk is a subset of the data, with a given kind and id - so a single time series.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    257\u001B[39m \u001B[33;03m    :rtype: pd.DataFrame\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m260\u001B[39m     data = \u001B[43mto_tsdata\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_kind\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_sort\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    262\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m distributor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    263\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, Iterable):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\.env\\master_project\\Lib\\site-packages\\tsfresh\\feature_extraction\\data.py:478\u001B[39m, in \u001B[36mto_tsdata\u001B[39m\u001B[34m(df, column_id, column_kind, column_value, column_sort)\u001B[39m\n\u001B[32m    476\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m WideTsFrameAdapter(df, column_id, column_sort, [column_value])\n\u001B[32m    477\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m478\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mWideTsFrameAdapter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_sort\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    480\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(df, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    481\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m TsDictAdapter(df, column_id, column_value, column_sort)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\.env\\master_project\\Lib\\site-packages\\tsfresh\\feature_extraction\\data.py:186\u001B[39m, in \u001B[36mWideTsFrameAdapter.__init__\u001B[39m\u001B[34m(self, df, column_id, column_sort, value_columns)\u001B[39m\n\u001B[32m    183\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m value_columns:\n\u001B[32m    184\u001B[39m     value_columns = _get_value_columns(df, column_id, column_sort)\n\u001B[32m--> \u001B[39m\u001B[32m186\u001B[39m \u001B[43m_check_nan\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mvalue_columns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    187\u001B[39m _check_colname(*value_columns)\n\u001B[32m    189\u001B[39m \u001B[38;5;28mself\u001B[39m.value_columns = value_columns\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\.env\\master_project\\Lib\\site-packages\\tsfresh\\feature_extraction\\data.py:145\u001B[39m, in \u001B[36m_check_nan\u001B[39m\u001B[34m(df, *columns)\u001B[39m\n\u001B[32m    142\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mColumn not found: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(col))\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m df[col].isnull().any():\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mColumn must not contain NaN values: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(col))\n",
      "\u001B[31mValueError\u001B[39m: Column must not contain NaN values: cob mean"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NightAnalyser:\n",
    "    def __init__(self, df, time_column_name='timestamp', feature_settings='comprehensive'):\n",
    "        \"\"\"\n",
    "        Initialises the NightAnalyser with the preprocessed time series data. It is assumed the DataFrame has a MultiIndex with 'id' and 'datetime', of night periods with consistent and complete intervals between a consistent start and end time.\n",
    "        :param df: Pandas DataFrame containing the time series data.\n",
    "        :param feature_settings: str, 'comprehensive', 'efficient', 'minimal', or 'custom'. Defines tsfresh feature extraction settings.\n",
    "        \"\"\"\n",
    "        df = check_df_index(df)  # Ensure the DataFrame has a MultiIndex with 'id' and 'datetime'\n",
    "\n",
    "        self.df = df.copy()\n",
    "        self.time_col = time_column_name\n",
    "        self.feature_settings = self._get_feature_settings(feature_settings)\n",
    "        self.night_features_df = None\n",
    "        self.scaled_night_features = None\n",
    "        self.night_pca_components = None\n",
    "        self.night_clusters = None\n",
    "        self.rolling_features_df = None\n",
    "\n",
    "        # Store scalers and PCA models\n",
    "        self.scaler = None\n",
    "        self.pca_model = None\n",
    "\n",
    "    def _get_feature_settings(self, setting_name):\n",
    "        \"\"\"Helper to get tsfresh feature extraction settings.\"\"\"\n",
    "        if setting_name == 'comprehensive':\n",
    "            return ComprehensiveFCParameters()\n",
    "        elif setting_name == 'efficient':\n",
    "            return EfficientFCParameters()\n",
    "        elif setting_name == 'minimal':\n",
    "            return MinimalFCParameters()\n",
    "        elif setting_name == 'custom':\n",
    "            return {\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"Invalid feature_settings. Choose 'comprehensive', 'efficient', 'minimal', or 'custom'.\")\n",
    "\n",
    "    def extract_night_level_features(self):\n",
    "        \"\"\"\n",
    "        Extracts aggregated tsfresh features for each complete night period.\n",
    "        The MultiIndex needs a unique 'night_id' for each night (e.g., id + datetime).\n",
    "        \"\"\"\n",
    "        print(f\"Extracting night-level features using {self.feature_settings.__class__.__name__} settings...\")\n",
    "\n",
    "        # Create a 'night_id' column for tsfresh. This assumes your MultiIndex already separates nights.\n",
    "        # If your MultiIndex level 0 is 'individual_id' and you have multiple nights per individual,\n",
    "        # you'll need to create a unique identifier for each *night*.\n",
    "        # Example: if MultiIndex is (individual_id, timestamp), extract date from timestamp.\n",
    "        temp_df = self.df.reset_index()\n",
    "        temp_df['night_date'] = temp_df['datetime'].dt.date\n",
    "        temp_df['night_id'] = temp_df['id'].astype(str) + '_' + temp_df['night_date'].astype(str)\n",
    "\n",
    "        # Set the night_id as the primary id for tsfresh extraction\n",
    "        self.night_features_df = extract_features(\n",
    "            temp_df.drop(columns=['night_date']), # Drop temporary night_date column\n",
    "            column_id='night_id',\n",
    "            column_sort=self.time_col,\n",
    "            default_fc_parameters=self.feature_settings,\n",
    "            impute_function=impute, # Apply imputation\n",
    "            show_warnings=True\n",
    "        )\n",
    "        print(f\"Extracted {self.night_features_df.shape[1]} features for {self.night_features_df.shape[0]} nights.\")\n",
    "        return self.night_features_df\n",
    "\n",
    "    def preprocess_night_features(self, n_components=0.95):\n",
    "        \"\"\"\n",
    "        Scales features and applies PCA for dimensionality reduction.\n",
    "\n",
    "        Args:\n",
    "            n_components (float or int): Number of PCA components or variance explained (0-1.0).\n",
    "        \"\"\"\n",
    "        if self.night_features_df is None:\n",
    "            raise ValueError(\"Night features not extracted yet. Run extract_night_level_features first.\")\n",
    "\n",
    "        print(\"Preprocessing night-level features (scaling and PCA)...\")\n",
    "\n",
    "        # Handle NaNs from tsfresh. You might prefer `dropna(axis=1)` if too many NaNs in a column.\n",
    "        # Using impute again to catch any new NaNs from feature extraction.\n",
    "        X_imputed = impute(self.night_features_df.copy())\n",
    "\n",
    "        # Drop columns with zero variance after imputation (can cause issues with StandardScaler)\n",
    "        X_imputed = X_imputed.loc[:, X_imputed.var() != 0]\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaled_night_features = self.scaler.fit_transform(X_imputed)\n",
    "        self.scaled_night_features = pd.DataFrame(\n",
    "            self.scaled_night_features,\n",
    "            columns=X_imputed.columns,\n",
    "            index=X_imputed.index\n",
    "        )\n",
    "\n",
    "        if n_components is not None:\n",
    "            self.pca_model = PCA(n_components=n_components)\n",
    "            self.night_pca_components = self.pca_model.fit_transform(self.scaled_night_features)\n",
    "            print(f\"PCA reduced dimensions from {self.scaled_night_features.shape[1]} to {self.night_pca_components.shape[1]}.\")\n",
    "            return self.night_pca_components\n",
    "        else:\n",
    "            return self.scaled_night_features\n",
    "\n",
    "    def cluster_nights(self, n_clusters, plot_2d=True):\n",
    "        \"\"\"\n",
    "        Clusters the nights using K-Means.\n",
    "\n",
    "        Args:\n",
    "            n_clusters (int): Number of clusters for K-Means.\n",
    "            plot_2d (bool): Whether to plot 2D PCA for clusters.\n",
    "        \"\"\"\n",
    "        if self.night_pca_components is None and self.scaled_night_features is None:\n",
    "            raise ValueError(\"Features not preprocessed yet. Run preprocess_night_features first.\")\n",
    "\n",
    "        data_for_clustering = self.night_pca_components if self.night_pca_components is not None else self.scaled_night_features.values\n",
    "        if data_for_clustering.shape[0] < n_clusters:\n",
    "             raise ValueError(f\"Number of nights ({data_for_clustering.shape[0]}) is less than n_clusters ({n_clusters}). Cannot cluster.\")\n",
    "\n",
    "\n",
    "        print(f\"Clustering nights into {n_clusters} clusters...\")\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) # n_init for robustness\n",
    "        self.night_clusters = kmeans.fit_predict(data_for_clustering)\n",
    "\n",
    "        self.night_features_df['cluster_label'] = self.night_clusters\n",
    "        print(\"Night cluster distribution:\")\n",
    "        print(self.night_features_df['cluster_label'].value_counts())\n",
    "\n",
    "        if plot_2d and self.night_pca_components is not None and self.night_pca_components.shape[1] >= 2:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.scatterplot(\n",
    "                x=self.night_pca_components[:, 0],\n",
    "                y=self.night_pca_components[:, 1],\n",
    "                hue=self.night_clusters,\n",
    "                palette='viridis',\n",
    "                alpha=0.7\n",
    "            )\n",
    "            plt.title(f'Nights Clustered (KMeans, K={n_clusters})')\n",
    "            plt.xlabel('Principal Component 1')\n",
    "            plt.ylabel('Principal Component 2')\n",
    "            plt.show()\n",
    "        elif plot_2d and (self.night_pca_components is None or self.night_pca_components.shape[1] < 2):\n",
    "            print(\"Cannot plot 2D PCA: PCA not performed or less than 2 components.\")\n",
    "\n",
    "        return self.night_clusters\n",
    "\n",
    "    def get_cluster_centroids(self):\n",
    "        \"\"\"Returns the mean feature values for each cluster (in original feature space).\"\"\"\n",
    "        if self.night_clusters is None:\n",
    "            raise ValueError(\"Nights not clustered yet. Run cluster_nights first.\")\n",
    "\n",
    "        # Inverse transform scaled features before averaging for interpretability\n",
    "        original_features_df = pd.DataFrame(\n",
    "            self.scaler.inverse_transform(self.scaled_night_features),\n",
    "            columns=self.scaled_night_features.columns,\n",
    "            index=self.scaled_night_features.index\n",
    "        )\n",
    "        original_features_df['cluster_label'] = self.night_clusters\n",
    "        return original_features_df.groupby('cluster_label').mean()\n",
    "\n",
    "    def extract_rolling_window_features(self, window_size='1H', overlap=0.5):\n",
    "        \"\"\"\n",
    "        Extracts tsfresh features from rolling windows within each original night. These features are suitable for HMM observations.\n",
    "        :param window_size: (str), Rolling window size (e.g., '30min', '1H').\n",
    "            overlap (float): Overlap between consecutive windows (0.0 to 1.0).\n",
    "        \"\"\"\n",
    "        print(f\"Extracting rolling window features (window={window_size}, overlap={overlap})...\")\n",
    "\n",
    "        df_flat = self.df.reset_index()\n",
    "\n",
    "        df_flat['night_id_temp'] = df_flat[self.id_col].astype(str) + '_' + df_flat[self.time_col].dt.date.astype(str)\n",
    "\n",
    "        rolled_df = roll_time_series(\n",
    "            df_flat,\n",
    "            column_id=\"night_id_temp\", # Each night is its own ID for rolling\n",
    "            column_sort=self.time_col,\n",
    "            min_timeseries_length=pd.Timedelta(window_size), # Convert string to Timedelta\n",
    "            max_timeseries_length=pd.Timedelta(window_size),\n",
    "            rolling_direction=1, # Roll forward\n",
    "            # This is where the overlap happens:\n",
    "            # We need to calculate step based on window_size and overlap\n",
    "            # A 1-hour window with 0.5 overlap means step is 0.5 hours.\n",
    "            # Convert window_size string to Timedelta for calculation\n",
    "            rolling_direction_in_consideration = pd.Timedelta(window_size) * (1 - overlap) # This is effectively the step\n",
    "        )\n",
    "\n",
    "        print(f\"Rolled into {len(rolled_df['night_id_temp'].unique())} unique night-windows.\")\n",
    "\n",
    "\n",
    "        # Now extract features from the rolled segments\n",
    "        self.rolling_features_df = extract_features(\n",
    "            rolled_df.drop(columns=['night_id_temp']), # temp night ID is now 'id' for extract_features\n",
    "            column_id='id', # This is the internal ID created by roll_time_series for each segment\n",
    "            column_sort=self.time_col,\n",
    "            default_fc_parameters=self.feature_settings,\n",
    "            impute_function=impute,\n",
    "            show_warnings=True\n",
    "        )\n",
    "\n",
    "        # The index of rolling_features_df will be a MultiIndex: (original_night_id, end_of_window_timestamp)\n",
    "        # Example: (('ind1_2018-03-16', Timestamp('2018-03-16 21:30:00')), ...)\n",
    "        # You'll likely want to extract the original night_id and window timestamp for later use.\n",
    "        # The 'id' column from roll_time_series will be the first level of the index.\n",
    "        # The 'time' column will be the second level of the index.\n",
    "\n",
    "        # Let's rename the index levels for clarity\n",
    "        self.rolling_features_df.index.set_names(['original_night_id', 'window_end_time'], inplace=True)\n",
    "\n",
    "        print(f\"Extracted {self.rolling_features_df.shape[1]} features for {self.rolling_features_df.shape[0]} rolling windows.\")\n",
    "        return self.rolling_features_df\n",
    "\n",
    "    def get_hmm_ready_data(self):\n",
    "        \"\"\"\n",
    "        Prepares the rolling window features for HMM training.\n",
    "        :return: Dictionary where keys are cluster labels and values are lists of arrays (sequences of feature vectors) for HMM training.\n",
    "        \"\"\"\n",
    "        if self.rolling_features_df is None or self.night_clusters is None:\n",
    "            raise ValueError(\"Rolling features not extracted or nights not clustered. Run respective methods first.\")\n",
    "\n",
    "        hmm_data_by_cluster = {cluster_id: [] for cluster_id in np.unique(self.night_clusters)}\n",
    "\n",
    "        # Get original night IDs and their assigned clusters\n",
    "        night_to_cluster_map = self.night_features_df['cluster_label'].to_dict()\n",
    "\n",
    "        # Iterate through the rolling features, group by original night, and assign to cluster\n",
    "        for original_night_id, group_df in self.rolling_features_df.groupby(level='original_night_id'):\n",
    "            cluster_id = night_to_cluster_map.get(original_night_id)\n",
    "            if cluster_id is not None:\n",
    "                # Ensure the sequence is sorted by time for HMM\n",
    "                sequence_data = group_df.sort_index(level='window_end_time').values\n",
    "                hmm_data_by_cluster[cluster_id].append(sequence_data)\n",
    "            else:\n",
    "                print(f\"Warning: Original night ID {original_night_id} not found in clustered nights. Skipping.\")\n",
    "\n",
    "        # Scale rolling features (important for HMMs too, often with StandardScaler)\n",
    "        # You might want a separate scaler for rolling features, or fit one globally\n",
    "        # to all rolling features\n",
    "        scaler_rolling = StandardScaler()\n",
    "        # Flatten all sequences to fit the scaler, then transform them back\n",
    "        all_sequences_flat = np.vstack([seq for sequences in hmm_data_by_cluster.values() for seq in sequences])\n",
    "        scaler_rolling.fit(all_sequences_flat)\n",
    "\n",
    "        for cluster_id, sequences in hmm_data_by_cluster.items():\n",
    "            hmm_data_by_cluster[cluster_id] = [scaler_rolling.transform(seq) for seq in sequences]\n",
    "\n",
    "        return hmm_data_by_cluster\n",
    "\n",
    "\n",
    "#    # If your multiindex isn't named, you'd need to set them for the class constructor\n",
    "#    # df.index.names = ['individual_id', 'timestamp']\n",
    "\n",
    "# Let's create a dummy DataFrame that matches your description for demonstration\n",
    "\n",
    "ids = [221634]*300 + [99908129]*100 + [12345]*100 # 3 nights for ind 221634, 1 for 99908129, 1 for 12345\n",
    "data = np.random.rand(500, 15).astype(np.float32)\n",
    "# Introduce some NaNs for demonstration (mimics your non-null counts)\n",
    "data[:, [0, 3, 6, 9]] = np.where(np.random.rand(500, 4) < 0.05, np.nan, data[:, [0, 3, 6, 9]])\n",
    "data[:, [1, 4, 7, 10]] = np.where(np.random.rand(500, 4) < 0.1, np.nan, data[:, [1, 4, 7, 10]])\n",
    "# Ensure counts are integers\n",
    "data[:, 12:15] = np.random.randint(1, 10, (500, 3))\n",
    "\n",
    "df_dummy = pd.DataFrame(data, columns=[\n",
    "    'iob mean', 'cob mean', 'bg mean', 'iob min', 'cob min', 'bg min',\n",
    "    'iob max', 'cob max', 'bg max', 'iob std', 'cob std', 'bg std',\n",
    "    'iob count', 'cob count', 'bg count'\n",
    "])\n",
    "df_dummy['individual_id'] = ids\n",
    "df_dummy['timestamp'] = dates\n",
    "\n",
    "df_dummy = df_dummy.set_index(['individual_id', 'timestamp'])\n",
    "df_dummy = df_dummy.sort_index() # Good practice for MultiIndex\n",
    "\n",
    "# Now use the class\n",
    "analyzer = NightAnalyser(df_dummy, id_column_name='individual_id', time_column_name='timestamp')\n",
    "\n",
    "# 2. Extract Night-Level Features\n",
    "night_features = analyzer.extract_night_level_features()\n",
    "print(\"\\nNight-level features extracted:\")\n",
    "print(night_features.head())\n",
    "\n",
    "# 3. Preprocess Night-Level Features (Scale and PCA)\n",
    "#    You might want to iterate on n_components for PCA by checking explained variance ratio\n",
    "#    plt.plot(np.cumsum(analyzer.pca_model.explained_variance_ratio_))\n",
    "#    plt.xlabel('number of components')\n",
    "#    plt.ylabel('cumulative explained variance')\n",
    "#    plt.show()\n",
    "pca_features = analyzer.preprocess_night_features(n_components=0.95)\n",
    "print(\"\\nPreprocessed PCA features for nights:\")\n",
    "print(pca_features[:5])\n",
    "\n",
    "# 4. Cluster Nights\n",
    "#    You'll typically use the Elbow Method/Silhouette Score (as shown in previous answer)\n",
    "#    to determine the optimal n_clusters before running this.\n",
    "n_clusters_chosen = 3 # Example\n",
    "analyzer.cluster_nights(n_clusters=n_clusters_chosen)\n",
    "print(\"\\nNight clusters assigned.\")\n",
    "\n",
    "# Get characteristics of clusters\n",
    "cluster_centroids = analyzer.get_cluster_centroids()\n",
    "print(\"\\nCluster Centroids (mean feature values in original scale):\")\n",
    "print(cluster_centroids)\n",
    "\n",
    "# 5. Extract Rolling Window Features for HMM\n",
    "#    Window size and overlap are critical here. Choose based on biological/clinical relevance\n",
    "#    and desired temporal resolution for your HMM states.\n",
    "#    e.g., if states change every 15-30 minutes, a 30min or 1H window with overlap is good.\n",
    "rolling_features = analyzer.extract_rolling_window_features(window_size='1H', overlap=0.5)\n",
    "print(\"\\nRolling window features extracted:\")\n",
    "print(rolling_features.head())\n",
    "\n",
    "# 6. Prepare data for HMM\n",
    "hmm_data = analyzer.get_hmm_ready_data()\n",
    "print(\"\\nData prepared for HMM training by cluster:\")\n",
    "for cluster_id, sequences in hmm_data.items():\n",
    "    print(f\"  Cluster {cluster_id}: {len(sequences)} sequences, e.g., first sequence shape: {sequences[0].shape}\")\n",
    "\n",
    "# Now you would typically iterate through hmm_data.items() and train an HMM for each cluster_id\n",
    "# using a library like `hmmlearn`."
   ],
   "id": "bc162d572c602e2e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
