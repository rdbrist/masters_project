{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tsfresh import extract_features, extract_relevant_features, feature_selection\n",
    "from tsfresh.utilities.dataframe_functions import impute, roll_time_series\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters, EfficientFCParameters\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from src.helper import check_df_index\n",
    "\n",
    "class NightAnalyser:\n",
    "    def __init__(self, df, id_column_name='individual_id', time_column_name='timestamp', feature_settings='comprehensive'):\n",
    "        \"\"\"\n",
    "        Initializes the NightAnalyser with the preprocessed time series data. It is assumed the DataFrame has a MultiIndex with 'id' and 'datetime', of night periods with consistent and complete intervals between a consistent start and end time.\n",
    "        :param df: Pandas DataFrame containing the time series data.\n",
    "        :param feature_settings: str, 'comprehensive', 'efficient', 'minimal', or 'custom'. Defines tsfresh feature extraction settings.\n",
    "        \"\"\"\n",
    "        df = check_df_index(df)  # Ensure the DataFrame has a MultiIndex with 'id' and 'datetime'\n",
    "\n",
    "        self.df = df.copy()\n",
    "        self.time_col = time_column_name\n",
    "        self.feature_settings = self._get_feature_settings(feature_settings)\n",
    "        self.night_features_df = None\n",
    "        self.scaled_night_features = None\n",
    "        self.night_pca_components = None\n",
    "        self.night_clusters = None\n",
    "        self.rolling_features_df = None\n",
    "\n",
    "        # Store scalers and PCA models\n",
    "        self.scaler = None\n",
    "        self.pca_model = None\n",
    "\n",
    "    def _get_feature_settings(self, setting_name):\n",
    "        \"\"\"Helper to get tsfresh feature extraction settings.\"\"\"\n",
    "        if setting_name == 'comprehensive':\n",
    "            return ComprehensiveFCParameters()\n",
    "        elif setting_name == 'efficient':\n",
    "            return EfficientFCParameters()\n",
    "        elif setting_name == 'minimal':\n",
    "            return MinimalFCParameters()\n",
    "        elif setting_name == 'custom':\n",
    "            return {\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"Invalid feature_settings. Choose 'comprehensive', 'efficient', 'minimal', or 'custom'.\")\n",
    "\n",
    "    def extract_night_level_features(self):\n",
    "        \"\"\"\n",
    "        Extracts aggregated tsfresh features for each complete night period.\n",
    "        The MultiIndex needs a unique 'night_id' for each night (e.g., individual_id + date).\n",
    "        \"\"\"\n",
    "        print(f\"Extracting night-level features using {self.feature_settings.__class__.__name__} settings...\")\n",
    "\n",
    "        # Create a 'night_id' column for tsfresh. This assumes your MultiIndex already separates nights.\n",
    "        # If your MultiIndex level 0 is 'individual_id' and you have multiple nights per individual,\n",
    "        # you'll need to create a unique identifier for each *night*.\n",
    "        # Example: if MultiIndex is (individual_id, timestamp), extract date from timestamp.\n",
    "        temp_df = self.df.reset_index()\n",
    "        temp_df['night_date'] = temp_df['datetime'].dt.date\n",
    "        temp_df['night_id'] = temp_df['id'].astype(str) + '_' + temp_df['night_date'].astype(str)\n",
    "\n",
    "        # Set the night_id as the primary id for tsfresh extraction\n",
    "        self.night_features_df = extract_features(\n",
    "            temp_df.drop(columns=['night_date']), # Drop temporary night_date column\n",
    "            column_id='night_id',\n",
    "            column_sort=self.time_col,\n",
    "            default_fc_parameters=self.feature_settings,\n",
    "            impute_function=impute, # Apply imputation\n",
    "            show_warnings=True\n",
    "        )\n",
    "        print(f\"Extracted {self.night_features_df.shape[1]} features for {self.night_features_df.shape[0]} nights.\")\n",
    "        return self.night_features_df\n",
    "\n",
    "    def preprocess_night_features(self, n_components=0.95):\n",
    "        \"\"\"\n",
    "        Scales features and applies PCA for dimensionality reduction.\n",
    "\n",
    "        Args:\n",
    "            n_components (float or int): Number of PCA components or variance explained (0-1.0).\n",
    "        \"\"\"\n",
    "        if self.night_features_df is None:\n",
    "            raise ValueError(\"Night features not extracted yet. Run extract_night_level_features first.\")\n",
    "\n",
    "        print(\"Preprocessing night-level features (scaling and PCA)...\")\n",
    "\n",
    "        # Handle NaNs from tsfresh. You might prefer `dropna(axis=1)` if too many NaNs in a column.\n",
    "        # Using impute again to catch any new NaNs from feature extraction.\n",
    "        X_imputed = impute(self.night_features_df.copy())\n",
    "\n",
    "        # Drop columns with zero variance after imputation (can cause issues with StandardScaler)\n",
    "        X_imputed = X_imputed.loc[:, X_imputed.var() != 0]\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaled_night_features = self.scaler.fit_transform(X_imputed)\n",
    "        self.scaled_night_features = pd.DataFrame(\n",
    "            self.scaled_night_features,\n",
    "            columns=X_imputed.columns,\n",
    "            index=X_imputed.index\n",
    "        )\n",
    "\n",
    "        if n_components is not None:\n",
    "            self.pca_model = PCA(n_components=n_components)\n",
    "            self.night_pca_components = self.pca_model.fit_transform(self.scaled_night_features)\n",
    "            print(f\"PCA reduced dimensions from {self.scaled_night_features.shape[1]} to {self.night_pca_components.shape[1]}.\")\n",
    "            return self.night_pca_components\n",
    "        else:\n",
    "            return self.scaled_night_features\n",
    "\n",
    "    def cluster_nights(self, n_clusters, plot_2d=True):\n",
    "        \"\"\"\n",
    "        Clusters the nights using K-Means.\n",
    "\n",
    "        Args:\n",
    "            n_clusters (int): Number of clusters for K-Means.\n",
    "            plot_2d (bool): Whether to plot 2D PCA for clusters.\n",
    "        \"\"\"\n",
    "        if self.night_pca_components is None and self.scaled_night_features is None:\n",
    "            raise ValueError(\"Features not preprocessed yet. Run preprocess_night_features first.\")\n",
    "\n",
    "        data_for_clustering = self.night_pca_components if self.night_pca_components is not None else self.scaled_night_features.values\n",
    "        if data_for_clustering.shape[0] < n_clusters:\n",
    "             raise ValueError(f\"Number of nights ({data_for_clustering.shape[0]}) is less than n_clusters ({n_clusters}). Cannot cluster.\")\n",
    "\n",
    "\n",
    "        print(f\"Clustering nights into {n_clusters} clusters...\")\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) # n_init for robustness\n",
    "        self.night_clusters = kmeans.fit_predict(data_for_clustering)\n",
    "\n",
    "        self.night_features_df['cluster_label'] = self.night_clusters\n",
    "        print(\"Night cluster distribution:\")\n",
    "        print(self.night_features_df['cluster_label'].value_counts())\n",
    "\n",
    "        if plot_2d and self.night_pca_components is not None and self.night_pca_components.shape[1] >= 2:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.scatterplot(\n",
    "                x=self.night_pca_components[:, 0],\n",
    "                y=self.night_pca_components[:, 1],\n",
    "                hue=self.night_clusters,\n",
    "                palette='viridis',\n",
    "                alpha=0.7\n",
    "            )\n",
    "            plt.title(f'Nights Clustered (KMeans, K={n_clusters})')\n",
    "            plt.xlabel('Principal Component 1')\n",
    "            plt.ylabel('Principal Component 2')\n",
    "            plt.show()\n",
    "        elif plot_2d and (self.night_pca_components is None or self.night_pca_components.shape[1] < 2):\n",
    "            print(\"Cannot plot 2D PCA: PCA not performed or less than 2 components.\")\n",
    "\n",
    "        return self.night_clusters\n",
    "\n",
    "    def get_cluster_centroids(self):\n",
    "        \"\"\"Returns the mean feature values for each cluster (in original feature space).\"\"\"\n",
    "        if self.night_clusters is None:\n",
    "            raise ValueError(\"Nights not clustered yet. Run cluster_nights first.\")\n",
    "\n",
    "        # Inverse transform scaled features before averaging for interpretability\n",
    "        original_features_df = pd.DataFrame(\n",
    "            self.scaler.inverse_transform(self.scaled_night_features),\n",
    "            columns=self.scaled_night_features.columns,\n",
    "            index=self.scaled_night_features.index\n",
    "        )\n",
    "        original_features_df['cluster_label'] = self.night_clusters\n",
    "        return original_features_df.groupby('cluster_label').mean()\n",
    "\n",
    "    def extract_rolling_window_features(self, window_size='1H', overlap=0.5):\n",
    "        \"\"\"\n",
    "        Extracts tsfresh features from rolling windows within each original night. These features are suitable for HMM observations.\n",
    "        :param window_size: (str), Rolling window size (e.g., '30min', '1H').\n",
    "            overlap (float): Overlap between consecutive windows (0.0 to 1.0).\n",
    "        \"\"\"\n",
    "        print(f\"Extracting rolling window features (window={window_size}, overlap={overlap})...\")\n",
    "\n",
    "        df_flat = self.df.reset_index()\n",
    "\n",
    "        df_flat['night_id_temp'] = df_flat[self.id_col].astype(str) + '_' + df_flat[self.time_col].dt.date.astype(str)\n",
    "\n",
    "        rolled_df = roll_time_series(\n",
    "            df_flat,\n",
    "            column_id=\"night_id_temp\", # Each night is its own ID for rolling\n",
    "            column_sort=self.time_col,\n",
    "            min_timeseries_length=pd.Timedelta(window_size), # Convert string to Timedelta\n",
    "            max_timeseries_length=pd.Timedelta(window_size),\n",
    "            rolling_direction=1, # Roll forward\n",
    "            # This is where the overlap happens:\n",
    "            # We need to calculate step based on window_size and overlap\n",
    "            # A 1-hour window with 0.5 overlap means step is 0.5 hours.\n",
    "            # Convert window_size string to Timedelta for calculation\n",
    "            rolling_direction_in_consideration = pd.Timedelta(window_size) * (1 - overlap) # This is effectively the step\n",
    "        )\n",
    "\n",
    "        print(f\"Rolled into {len(rolled_df['night_id_temp'].unique())} unique night-windows.\")\n",
    "\n",
    "\n",
    "        # Now extract features from the rolled segments\n",
    "        self.rolling_features_df = extract_features(\n",
    "            rolled_df.drop(columns=['night_id_temp']), # temp night ID is now 'id' for extract_features\n",
    "            column_id='id', # This is the internal ID created by roll_time_series for each segment\n",
    "            column_sort=self.time_col,\n",
    "            default_fc_parameters=self.feature_settings,\n",
    "            impute_function=impute,\n",
    "            show_warnings=True\n",
    "        )\n",
    "\n",
    "        # The index of rolling_features_df will be a MultiIndex: (original_night_id, end_of_window_timestamp)\n",
    "        # Example: (('ind1_2018-03-16', Timestamp('2018-03-16 21:30:00')), ...)\n",
    "        # You'll likely want to extract the original night_id and window timestamp for later use.\n",
    "        # The 'id' column from roll_time_series will be the first level of the index.\n",
    "        # The 'time' column will be the second level of the index.\n",
    "\n",
    "        # Let's rename the index levels for clarity\n",
    "        self.rolling_features_df.index.set_names(['original_night_id', 'window_end_time'], inplace=True)\n",
    "\n",
    "        print(f\"Extracted {self.rolling_features_df.shape[1]} features for {self.rolling_features_df.shape[0]} rolling windows.\")\n",
    "        return self.rolling_features_df\n",
    "\n",
    "    def get_hmm_ready_data(self):\n",
    "        \"\"\"\n",
    "        Prepares the rolling window features for HMM training.\n",
    "        :return: Dictionary where keys are cluster labels and values are lists of arrays (sequences of feature vectors) for HMM training.\n",
    "        \"\"\"\n",
    "        if self.rolling_features_df is None or self.night_clusters is None:\n",
    "            raise ValueError(\"Rolling features not extracted or nights not clustered. Run respective methods first.\")\n",
    "\n",
    "        hmm_data_by_cluster = {cluster_id: [] for cluster_id in np.unique(self.night_clusters)}\n",
    "\n",
    "        # Get original night IDs and their assigned clusters\n",
    "        night_to_cluster_map = self.night_features_df['cluster_label'].to_dict()\n",
    "\n",
    "        # Iterate through the rolling features, group by original night, and assign to cluster\n",
    "        for original_night_id, group_df in self.rolling_features_df.groupby(level='original_night_id'):\n",
    "            cluster_id = night_to_cluster_map.get(original_night_id)\n",
    "            if cluster_id is not None:\n",
    "                # Ensure the sequence is sorted by time for HMM\n",
    "                sequence_data = group_df.sort_index(level='window_end_time').values\n",
    "                hmm_data_by_cluster[cluster_id].append(sequence_data)\n",
    "            else:\n",
    "                print(f\"Warning: Original night ID {original_night_id} not found in clustered nights. Skipping.\")\n",
    "\n",
    "        # Scale rolling features (important for HMMs too, often with StandardScaler)\n",
    "        # You might want a separate scaler for rolling features, or fit one globally\n",
    "        # to all rolling features\n",
    "        scaler_rolling = StandardScaler()\n",
    "        # Flatten all sequences to fit the scaler, then transform them back\n",
    "        all_sequences_flat = np.vstack([seq for sequences in hmm_data_by_cluster.values() for seq in sequences])\n",
    "        scaler_rolling.fit(all_sequences_flat)\n",
    "\n",
    "        for cluster_id, sequences in hmm_data_by_cluster.items():\n",
    "            hmm_data_by_cluster[cluster_id] = [scaler_rolling.transform(seq) for seq in sequences]\n",
    "\n",
    "        return hmm_data_by_cluster\n",
    "\n",
    "\n",
    "#    # If your multiindex isn't named, you'd need to set them for the class constructor\n",
    "#    # df.index.names = ['individual_id', 'timestamp']\n",
    "\n",
    "# Let's create a dummy DataFrame that matches your description for demonstration\n",
    "\n",
    "ids = [221634]*300 + [99908129]*100 + [12345]*100 # 3 nights for ind 221634, 1 for 99908129, 1 for 12345\n",
    "data = np.random.rand(500, 15).astype(np.float32)\n",
    "# Introduce some NaNs for demonstration (mimics your non-null counts)\n",
    "data[:, [0, 3, 6, 9]] = np.where(np.random.rand(500, 4) < 0.05, np.nan, data[:, [0, 3, 6, 9]])\n",
    "data[:, [1, 4, 7, 10]] = np.where(np.random.rand(500, 4) < 0.1, np.nan, data[:, [1, 4, 7, 10]])\n",
    "# Ensure counts are integers\n",
    "data[:, 12:15] = np.random.randint(1, 10, (500, 3))\n",
    "\n",
    "df_dummy = pd.DataFrame(data, columns=[\n",
    "    'iob mean', 'cob mean', 'bg mean', 'iob min', 'cob min', 'bg min',\n",
    "    'iob max', 'cob max', 'bg max', 'iob std', 'cob std', 'bg std',\n",
    "    'iob count', 'cob count', 'bg count'\n",
    "])\n",
    "df_dummy['individual_id'] = ids\n",
    "df_dummy['timestamp'] = dates\n",
    "\n",
    "df_dummy = df_dummy.set_index(['individual_id', 'timestamp'])\n",
    "df_dummy = df_dummy.sort_index() # Good practice for MultiIndex\n",
    "\n",
    "# Now use the class\n",
    "analyzer = NightAnalyser(df_dummy, id_column_name='individual_id', time_column_name='timestamp')\n",
    "\n",
    "# 2. Extract Night-Level Features\n",
    "night_features = analyzer.extract_night_level_features()\n",
    "print(\"\\nNight-level features extracted:\")\n",
    "print(night_features.head())\n",
    "\n",
    "# 3. Preprocess Night-Level Features (Scale and PCA)\n",
    "#    You might want to iterate on n_components for PCA by checking explained variance ratio\n",
    "#    plt.plot(np.cumsum(analyzer.pca_model.explained_variance_ratio_))\n",
    "#    plt.xlabel('number of components')\n",
    "#    plt.ylabel('cumulative explained variance')\n",
    "#    plt.show()\n",
    "pca_features = analyzer.preprocess_night_features(n_components=0.95)\n",
    "print(\"\\nPreprocessed PCA features for nights:\")\n",
    "print(pca_features[:5])\n",
    "\n",
    "# 4. Cluster Nights\n",
    "#    You'll typically use the Elbow Method/Silhouette Score (as shown in previous answer)\n",
    "#    to determine the optimal n_clusters before running this.\n",
    "n_clusters_chosen = 3 # Example\n",
    "analyzer.cluster_nights(n_clusters=n_clusters_chosen)\n",
    "print(\"\\nNight clusters assigned.\")\n",
    "\n",
    "# Get characteristics of clusters\n",
    "cluster_centroids = analyzer.get_cluster_centroids()\n",
    "print(\"\\nCluster Centroids (mean feature values in original scale):\")\n",
    "print(cluster_centroids)\n",
    "\n",
    "# 5. Extract Rolling Window Features for HMM\n",
    "#    Window size and overlap are critical here. Choose based on biological/clinical relevance\n",
    "#    and desired temporal resolution for your HMM states.\n",
    "#    e.g., if states change every 15-30 minutes, a 30min or 1H window with overlap is good.\n",
    "rolling_features = analyzer.extract_rolling_window_features(window_size='1H', overlap=0.5)\n",
    "print(\"\\nRolling window features extracted:\")\n",
    "print(rolling_features.head())\n",
    "\n",
    "# 6. Prepare data for HMM\n",
    "hmm_data = analyzer.get_hmm_ready_data()\n",
    "print(\"\\nData prepared for HMM training by cluster:\")\n",
    "for cluster_id, sequences in hmm_data.items():\n",
    "    print(f\"  Cluster {cluster_id}: {len(sequences)} sequences, e.g., first sequence shape: {sequences[0].shape}\")\n",
    "\n",
    "# Now you would typically iterate through hmm_data.items() and train an HMM for each cluster_id\n",
    "# using a library like `hmmlearn`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
